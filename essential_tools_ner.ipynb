{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing required modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Import BeautifulSoup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Import NLTK module\n",
    "import nltk\n",
    "\n",
    "# Import word_tokenize \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Import POS tagger\n",
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtaining text from a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# send a request to the website\n",
    "page = requests.get(\"https://en.wikipedia.org/wiki/Natural_Language_Toolkit\")\n",
    "\n",
    "# Use BeautifulSoup to parse HTML using html5 protocol. It is slower\n",
    "# but more efficient \n",
    "page_content = BeautifulSoup(page.text, \"html5lib\")\n",
    "\n",
    "# Now we look for the paragraphs\n",
    "textContent = []\n",
    "for i in range(0, 3):\n",
    "    paragraphs = page_content.find_all(\"p\")[i].text\n",
    "    textContent.append(paragraphs)\n",
    "\n",
    "# Join the paragraphs together and replace the `\\n` for empty strings\n",
    "wiki_nltk = \" \".join(textContent).replace(\"\\n\", \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The Natural Language Toolkit, or more commonly NLTK, is a suite of libraries and programs for symbolic and statistical natural language processing (NLP) for English written in the Python programming language. It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania.[5] NLTK includes graphical demonstrations and sample data. It is accompanied by a book that explains the underlying concepts behind the language processing tasks supported by the toolkit,[6] plus a cookbook.[7] NLTK is intended to support research and teaching in NLP or closely related areas, including empirical linguistics, cognitive science, artificial intelligence, information retrieval, and machine learning.[8]NLTK has been used successfully as a teaching tool, as an individual study tool, and as a platform for prototyping and building research systems. There are 32 universities in the US and 25 countries using NLTK in their courses. NLTK supports classification, tokenization, stemming, tagging, parsing, and semantic reasoning functionalities.[9]'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTLK in action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK is the most used platform when working with human language data in Python. It provides more than 50 corpora and lexical resources. It also has libraries to classify, tokenize, and tag texts, among other functions.\n",
    "It is able to help us with our task of classifying words in verbs, nouns, adjectives. Together they are referred to as parts of speech. And the task of labeling them is called part-of-speech tagging, or POS-tagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define a method that takes the text, tokenize it, label it and returns the tuples. Each tuple consists of a word and the tag with its part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    This function takes a text. Split it in tokens using word_tokenize. \n",
    "    And then tags them using pos_tag from NLTK module.\n",
    "    It outputs a list of tuples. Each tuple consists of a word and the tag with its \n",
    "    part of speech.\n",
    "    \"\"\"\n",
    "    # Get the tokens\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Tags the tokens\n",
    "    tagging = nltk.pos_tag(tokens)\n",
    "    # Returns the list of tuples\n",
    "    return tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we apply the method to the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split and label the text\n",
    "label_text = preprocess_text(wiki_nltk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we print the first 20 tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 'DT') ('Natural', 'NNP') ('Language', 'NNP') ('Toolkit', 'NNP') (',', ',')\n",
      "('or', 'CC') ('more', 'JJR') ('commonly', 'RB') ('NLTK', 'NNP') (',', ',')\n",
      "('is', 'VBZ') ('a', 'DT') ('suite', 'NN') ('of', 'IN') ('libraries', 'NNS')\n",
      "('and', 'CC') ('programs', 'NNS') ('for', 'IN') ('symbolic', 'JJ') ('and', 'CC')\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 20, 5):\n",
    "    print(label_text[i], label_text[i+1], label_text[i+2], label_text[i+3], label_text[i+4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
